---
title: "UTF-8 인코딩 구현하기"
date: 2023-06-09T21:00:00+09:00
summary: "문자와 코드 포인트에서 시작하는 UTF 인코딩 구현"
thumbnail: "./_figs/thumbnail.webp"
series: "유니코드와 UTF 인코딩 구현하기"
---

import AltLang from "@/components/post/AltLang.astro";
import FigureV2 from "@/components/post/FigureV2.astro";
import FigureCaption from "@/components/post/FigureCaption.astro";

import fig1 from "./_figs/fig1.svg";
import fig2 from "./_figs/fig2.svg";
import fig3 from "./_figs/fig3.svg";
import fig4 from "./_figs/fig4.svg";
import fig5 from "./_figs/fig5.svg";

유니코드<AltLang>Unicode</AltLang>와 UTF-8은 문자 인코딩에서 자주 마주치게 되는 용어입니다.

유닉스의 성공에 이어 벨 연구소에서 발표한 [Plan 9][wiki-plan-9]이란 운영체제는 당시 리눅스에 밀리고 말았지만 UTF-8이라는 유산을 남기고 갔습니다.
그리고 이 인코딩은 2023년에 이르러 웹에서 [약 98%의 점유율][w3tech-utf-8]을 가질 정도로 퍼지게 되었습니다.

이 글에서는 유니코드와 UTF-8 간의 관계를 간략하게 정리하고, UTF-8를 직접 구현해보며 자세히 살펴보겠습니다.
이를 통해 유니코드의 $2^{21}$개에 달하는 코드 포인트<AltLang>code point</AltLang>를 UTF-8로 완전하게 커버할 수 있음을 알 수 있게 됩니다.
그리고 유니코드의 플레인<AltLang>plane</AltLang>을 소개하는 것으로 유니코드를 간단히 이해해보겠습니다.

[wiki-plan-9]: https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs
[w3tech-utf-8]: https://w3techs.com/technologies/history_overview/character_encoding/ms/y


# 매핑으로서 바라보는 유니코드와 UTF 인코딩

문자 'A'가 있다고 생각해봅시다.
이것이 비트로 어떻게 표현되는지는 일단 머릿속에서 지우고요.
다시 말해 온전히 추상적인 심볼<AltLang>symbol</AltLang>로서 A를 떠올려봅시다.

유니코드는 이런 추상적인 심볼에서 비트 표현에 이르기까지 여러 단계로 구분하고 있는데요.
[Unicode Technical Report][utr-17-model]에서 볼 수 있듯이 네 단계의 레이어로 나누고 있습니다.
여기서는 대신 두 가지의 매핑<AltLang>mapping</AltLang>만 언급하려는데요.

[utr-17-model]: https://unicode.org/reports/tr17/#CharacterEncodingModel

<FigureV2 src={fig1} alt="unicode-utf mapping">
  <FigureCaption slot="caption">그림 1. 유니코드와 UTF-8 매핑. 유니코드는 문자마다 하나의 코드 포인트를, UTF-8 인코딩은 코드 포인트마다 하나의 비트 표현을 매깁니다.</FigureCaption>
</FigureV2>

하나는 문자를 숫자에 대응시키는 매핑입니다.
그 숫자는 코드 포인트<AltLang>code point</AltLang>라고 부릅니다.
유니코드는 이러한 매핑 중 하나인데요.

예를 들어, 유니코드는 그리스 문자 'α'에 945번이라는 코드 포인트를 매겨놓았습니다.
(뒤에서 보겠지만) 이것은 유니코드가 가질 수 있는 $2^{21}$개의 코드 포인트 중 하나입니다.
이는 16진수로 3B1번이 되는데요.
따라서 `U+03B1`로도 표기합니다.

또 다른 하나는 이 코드 포인트에서 구체적인 메모리 상의 비트 표현으로 (달리 말하면 코드 유닛 시퀀스<AltLang>code unit sequence</AltLang>로) 대응시키는 매핑이 있습니다.
UTF-8이 이러한 매핑 중 하나입니다.
(UTF-16과 UTF-32도 그런 매핑이고요.)
구체적인 매핑 방법은 직접 구현할 때 보도록 하겠습니다.

즉 유니코드는 문자와 코드 포인트 사이의 매핑이고, UTF 인코딩은 코드 포인트와 비트 표현 사이의 매핑입니다.


# 직접 구현해보는 UTF-8 인코딩

문자와 코드 포인트 간의 관계는 이미 유니코드에 정해져있습니다.

따라서 코드 포인트가 주어졌을 때 그것을 바이트 시퀀스<AltLang>byte sequence</AltLang>, 즉 바이트의 나열로 바꾸는 함수를 구현하면 됩니다.
이를 인코딩<AltLang>encoding</AltLang>이라고 부르도록 합시다.
그러면 UTF-8로 인코딩하는 방법 또한 이미 정해져있기 때문에 그것을 구현하기만 하면 됩니다.

그 인코딩 방법은 무엇일까요?

코드 포인트의 이진 표현<AltLang>binary representation</AltLang>이 몇 비트를 차지하느냐에 따라 케이스를 나누는데요.
7비트, 11비트, 16비트, 그리고 21비트가 기준점이 됩니다.
<FigureV2 src={fig2} alt="Utf-8 encoding">
  <FigureCaption slot="caption">그림 2. UTF-8 인코딩. 코드 포인트의 이진 표현 길이에 따라 UTF-8의 비트 표현이 달라집니다.</FigureCaption>
</FigureV2>
즉 필요한 비트가 많을수록 더 많은 바이트에 나눠 담습니다.

각 바이트의 앞자리에는 정해진 비트 패턴이 있는데요.
이것 덕분에 어느 바이트에서 읽기를 시작하든지 의미를 분명하게 알 수 있습니다.
즉 인코딩의 시작인지 중간인지 바로 알 수 있게 만들어 줍니다. ([셀프 싱크로나이징 코드][wiki-self-sync]<AltLang>self-synchronizing code</AltLang>라고도 부릅니다.)

<FigureV2 src={fig3} alt="prefixes in utf-8">
  <FigureCaption slot="caption">그림 3. UTF-8 바이트 비트 패턴. 앞에 붙은 비트 패턴이 그 바이트의 의미를 정합니다.</FigureCaption>
</FigureV2>

[wiki-self-sync]: https://en.wikipedia.org/wiki/Self-synchronizing_code

UTF-8는 네 개의 바이트까지 사용하면 $2^{21}$개의 문자를 표현할 수 있는데요.
마침 유니코드 또한 그만큼, 즉 $2^{21}$개의 코드 포인트를 명시하고 있습니다.
따라서 유니코드의 모든 코드 포인트, 다시 말해 모든 문자를 UTF-8로 표현할 수 있게 됩니다.

아래 수도코드<AltLang>pseudocode</AltLang>의 `encodeUtf8()` 함수는 코드 포인트를 나타내는 숫자를 입력받고 그것을 UTF-8로 인코딩한 바이트 시퀀스를 리턴합니다.

```
function encodeUtf8(codepoint)
  if codepoint is within 7 bits
    let xxx xxxx be the binary representation of codepoint
    return byte sequence [0xxx xxxx]

  if codepoint is within 11 bits
    let yyy yyxx xxxx be the binary representation of codepoint
    return byte sequence [110y yyyy, 10xx xxxx]

  if codepoint is within 16 bits
    let zzzz yyyy yyxx xxxx be the binary representation of codepoint
    return byte sequence [1110 zzzz, 10yy yyyy, 10xx xxxx]

  if codepoint is within 21 bits
    let w wwzz zzzz yyyy yyxx xxxx be the binary representation of codepoint
    return byte sequence [1111 1www, 10zz zzzz, 10yy yyyy, 10xx xxxx]

  return U+FFFD in UTF-8
```

맨 아래에서는 21비트를 넘어가는 코드 포인트를 만나는 예외적인 경우를 처리하는데요.
이때는 특별한 문자인 ‘&#65533;’를 리턴합니다.
`U+FFFD`로 정의된 이 문자는 [대체 문자][wiki-repl-char]<AltLang>replacement character</AltLang>라고 부르는데요.
표현할 수 없는 문자를 만나면 대신 사용하도록 만들어졌습니다.

[wiki-repl-char]: https://ko.wikipedia.org/wiki/%EB%8C%80%EC%B2%B4_%EB%AC%B8%EC%9E%90

## 이제 진짜 만들어보기

저수준 작업인만큼 여기서는 C++를 사용해보겠습니다.
하지만 다른 선호하는 언어로도 구현할 수 있을 것입니다.

`Codepoint` 타입을 32비트 정수 타입으로 선언하고, 앞서 소개한 수도코드대로 코드를 작성한 결과입니다.

```cpp
typedef uint32_t Codepoint;

void encodeUtf8(const Codepoint cp, char8_t *buf) {
  bool within_7bits = cp < (1 << 7);
  if (within_7bits) {
    buf[0] = cp;
    buf[1] = '\0';

    return;
  }

  bool within_11bits = cp < (1 << 11);
  if (within_11bits) {
    Byte y = (cp & 0b111'1100'0000) >> 6;
    Byte x = (cp & 0b000'0011'1111);

    buf[0] = 0b1100'0000 | y;
    buf[1] = 0b1000'0000 | x;
    buf[2] = '\0';

    return;
  }

  bool within_16bits = cp < (1 << 16);
  if (within_16bits) {
    Byte z = (cp & 0b1111'0000'0000'0000) >> 12;
    Byte y = (cp & 0b0000'1111'1100'0000) >> 6;
    Byte x = (cp & 0b0000'0000'0011'1111);

    buf[0] = 0b1110'0000 | z;
    buf[1] = 0b1000'0000 | y;
    buf[2] = 0b1000'0000 | x;
    buf[3] = '\0';

    return;
  }

  bool within_21bits = cp < (1 << 21);
  if (within_21bits) {
    Byte w = (cp & 0b1'1100'0000'0000'0000'0000) >> 18;
    Byte z = (cp & 0b0'0011'1111'0000'0000'0000) >> 12;
    Byte y = (cp & 0b0'0000'0000'1111'1100'0000) >> 6;
    Byte x = (cp & 0b0'0000'0000'0000'0011'1111);

    buf[0] = 0b1111'0000 | w;
    buf[1] = 0b1000'0000 | z;
    buf[2] = 0b1000'0000 | y;
    buf[3] = 0b1000'0000 | x;
    buf[4] = '\0';

    return;
  }

  buf[0] = 0xEF;
  buf[1] = 0xBF;
  buf[2] = 0xBD;
  buf[3] = '\0';
}
```

간단한 비트 조작을 통해 구현할 수 있는데요.
수도코드는 바이트 시퀀스를 리턴했지만, 여기서는 매개변수로 받은 `buf`에 사이드 이펙트<AltLang>side effect</AltLang>로 리턴값을 담습니다.

마지막에 널 문자 `\0`을 붙인 것은 나중에 출력의 편의를 위해 임의로 붙인 것입니다.

## 테스트해보기

이렇게 만든 함수는 `main` 함수에서 다음과 같이 테스트해볼 수 있는데요.
```cpp
int main() {
  char8_t buf[5];
  encodeUtf8(0x1F602, buf);
  printf("%s\n", buf); // result: 😂
}
```
위 코드는 UTF-8로 인코딩한 바이트 시퀀스를 터미널에 출력하도록 합니다.
여기서는 코드 포인트 `U+1F602`에 해당하는 문자 ‘😂’를 출력합니다.

여기서 터미널이 바이트 시퀀스를 이해하고 화면에 렌더하는 것은 별개의 일입니다.
터미널 자체에 인코딩 문제가 있을 수 있고요.
폰트가 그 코드 포인트에 해당하는 문자를 지원하지 않을 수도 있습니다.

리눅스에서는 보통 터미널이 UTF-8 인코딩을 네이티브하게 지원할 텐데요.
윈도우는 다음과 같은 헤더와 API 호출이 필요할 것입니다.
```cpp
#include <windows.h>

int main() {
  SetConsoleOutputCP(CP_UTF8);

  // ...
}
```


# 반대로 동작하는 디코딩 함수 구현하기

이번에는 바이트 시퀀스에서 코드 포인트를 얻는 `decodeUtf8()` 함수를 만들어 봅시다.
기존에 만든 `encodeUtf8()` 함수와 정반대의 작업을 할 것입니다.
즉 바이트 시퀀스를 받아서 코드 포인트를 계산해 리턴할 것입니다.

```
function decodeUtf8(sequence)
  if sequence is 0xxx xxxx
    return codepoint xxx xxxx

  if sequence is 110x xxxx 10yy yyyy
    return codepoint xxx xxyy yyyy

  if sequence is 1110 xxxx 10yy yyyy 10zz zzzz
    return codepoint xxxx yyyy yyzz zzzz

  if sequence is 1111 1xxx 10yy yyyy 10zz zzzz 10ww wwww
    return codepoint x xxyy yyyy zzzz zzww wwww

  return codepoint U+FFFD
```

그러면 C++로 구현해봅시다.

```cpp
Codepoint decodeUtf8(const char8_t *buf) {
  if ((buf[0] & 0b1000'0000) == 0) {
    // get xxx xxxx
    // from bytes 0xxx xxxx

    return (Codepoint) buf[0];
  }

  if (((buf[0] & 0b1110'0000) >> 5) == 0b110 &&
      ((buf[1] & 0b1100'0000) >> 6) == 0b10) {
    // get xxx xxyy yyyy
    // from bytes 110x xxxx 10yy yyyy

    Codepoint x = buf[0] &  0b1'1111;
    Codepoint y = buf[1] & 0b11'1111;

    Codepoint cp = (x << 6) | y;
    return cp;
  }

  if (((buf[0] & 0b1111'0000) >> 4) == 0b1110 &&
      ((buf[1] & 0b1100'0000) >> 6) == 0b10 &&
      ((buf[2] & 0b1100'0000) >> 6) == 0b10) {
    // get xxxx yyyy yyzz zzzz
    // from bytes 1110 xxxx 10yy yyyy 10zz zzzz

    Codepoint x = buf[0] &    0b1111;
    Codepoint y = buf[1] & 0b11'1111;
    Codepoint z = buf[2] & 0b11'1111;

    Codepoint cp = (x << 12) | (y << 6) | z;
    return cp;
  }

  if (((buf[0] & 0b1111'1000) >> 3) == 0b11110 &&
      ((buf[1] & 0b1100'0000) >> 6) == 0b10 &&
      ((buf[2] & 0b1100'0000) >> 6) == 0b10 &&
      ((buf[3] & 0b1100'0000) >> 6) == 0b10) {
    // get x xxyy yyyy zzzz zzww wwww
    // from bytes 1111 0xxx 10yy yyyy 10zz zzzz 10ww wwww

    Codepoint x = buf[0] &     0b111;
    Codepoint y = buf[1] & 0b11'1111;
    Codepoint z = buf[2] & 0b11'1111;
    Codepoint w = buf[3] & 0b11'1111;

    Codepoint cp = (x << 18) | (y << 12) | (z << 6) | w;
    return cp;
  }

  return 0xFFFD;
}
```

디코딩 함수는 다음과 같이 `main()` 함수에서 테스트해볼 수 있습니다.

```cpp
int main() {
  Codepoint cp = decodeUtf8(u8"😂");
  printf("%x\n", cp); // result: 1f602
}
```


# 유니코드의 코드 스페이스와 플레인

유니코드는 `U+0000`부터 `U+10FFFF`까지, 즉 $2^{21}$개의 코드 포인트를 사용합니다.
이런 코드 포인트의 범위를 코드 스페이스<AltLang>code space</AltLang>라고 부릅니다.

유니코드는 코드 스페이스를 열일곱개의 영역으로 나눴는데요. 이를 각각 [플레인][wiki-unicode-plane]<AltLang>plane</AltLang>이라고 부릅니다.
하나의 플레인은 $2^{16}$개, 즉 65536개의 코드포인트를 가지게 됩니다.

따라서 코드 포인트 `U+XXYYYY`가 있으면, `XX`는 `0`부터 `10`까지 있을 수 있는데요. 플레인의 숫자를 가리키게 됩니다.
예를 들어, `U+2345`는 `0`번째 플레인에 있는 코드 포인트가 되겠네요.

<FigureV2 src={fig4} alt="code space and planes">
  <FigureCaption slot="caption">그림 4. 유니코드의 코드 스페이스와 플레인. 코드 스페이스는 총 17개의 플레인으로 구성되어 있습니다. 그리고 각 플레인은 블록이라는 이름으로 구역을 나누고 있습니다.</FigureCaption>
</FigureV2>

원래 유니코드를 만들었을 때, 16비트로도 충분히 모든 문자를 표현할 수 있을거라고 예상했었는데요.
따라서 자주 사용되는 문자는 코드 포인트 `0+0000`와 `0+FFFF` 사이에 할당되어있습니다.
그리고 이에 해당하는 플레인을 다국어 기본 평면<AltLang>Basic Multilingual Plane (BMP)</AltLang>이라고 부르는데요.
실제로 유니코드의 버전 3.0까지는 이 플레인만 사용하고 있었습니다.
한글도 여기에 포함되어 있습니다.

이후 한자를 비롯한 여러 다른 문자를 포함하기 위해 별도의 플레인을 사용하기 시작했습니다.
이모지는 1번 플레인인 다국어 보충 평면<AltLang>Supplementary Multilingual Plane (SMP)</AltLang>을 사용하고 있습니다.
한자는 2번과 3번 플레인인 상형 문자 보충 평면<AltLang>Supplementary Ideographic Plane (SIP)</AltLang>, 상형 문자 제삼평면<AltLang>Tertiary Ideographic Plane (TIP)</AltLang>이라고 불리는 플레인을 쓰고 있습니다.
(여기서 이름이 중요한 건 아니지만요.)

유니코드 버전 15 기준으로 4번 플레인부터 13번 플레인까지는 비어있는 상태입니다.
그리고 나머지 플레인은 이 글의 주제를 벗어나므로 건너뛰도록 하겠습니다.

플레인 안에는 블록이라는 영역으로 나누고 있지만, 이 또한 주제를 벗어나기 때문에 언급만 하는 것으로 정리하겠습니다.

[wiki-unicode-plane]: https://ko.wikipedia.org/wiki/%EC%9C%A0%EB%8B%88%EC%BD%94%EB%93%9C_%ED%8F%89%EB%A9%B4


# 코드 유닛과 비트 표현 매핑

그런데 UTF-8의 뒤에 붙은 숫자 8은 무엇일까요?
코드 포인트를 비트 표현으로 매핑할 때, 기본 단위로서 사용할 비트 길이가 있는데요.
이를 코드 유닛<AltLang>code unit</AltLang>이라고 부릅니다.

예를 들어 8비트를 기본 단위로 사용한다면, 코드 포인트를 8비트의 코드 유닛 시퀀스<AltLang>code unit sequence</AltLang>로 인코딩한다고 부릅니다.
UTF-8은 그 방법 중 하나입니다.
앞서 살펴보았듯이, 7비트까지는 하나의 코드 유닛을, 11비트까지는 두 개의 코드 유닛을 사용하고 이런식으로 나아갑니다.

즉 UTF-8 인코딩은 8비트 코드 유닛을 때에 따라 하나에서 네 개까지 사용하는데요.
문자마다 코드 유닛의 개수, 즉 바이트의 개수가 달라지므로, 가변 폭 인코딩<AltLang>variable-width encoding</AltLang>이라고 부릅니다.

<FigureV2 src={fig5} alt="various utf encoding examples">
  <FigureCaption slot="caption">그림 5. 같은 문자의 세 UTF 인코딩. 뒤에 붙은 숫자, 즉 8, 16, 32는 기본 단위로서 사용할 비트 길이를 의미합니다.</FigureCaption>
</FigureV2>

그러면 16비트의 코드 유닛을 사용하면 어떨까요?
UTF-16이 그 방법 중 하나가 됩니다.
UTF-16도 때에 따라 하나에서 두 개의 코드 유닛을 쓰기 때문에 가변 폭 인코딩으로 분류됩니다.

그러면 UTF-32는요?
예상할 수 있듯이 32비트의 코드 유닛을 쓰는 인코딩 방식입니다.
하지만 UTF-32는 오직 하나의 코드 유닛만 사용하는데요.
$2^{21}$개의 모든 코드 포인트를 항상 32비트 안에 담을 수 있기 때문입니다.
그래서 고정 폭 인코딩<AltLang>fixed-width encoding</AltLang>이라고 부릅니다.
[ASCII][wiki-ascii]처럼요.

결론적으로 이모지 문자 ‘😂’는 세 가지 인코딩에서 모두 네 바이트를 사용하는데요.
각각 다른 개수의 코드 유닛으로 나누어 인코딩 됩니다.

[wiki-ascii]: https://ko.wikipedia.org/wiki/ASCII


# 그 외의 것들

여기까지 유니코드와 UTF-8에 대해 살펴보았습니다.

본문에서 다루지 않았던 내용을 짤막하게 언급하고 넘어가자면 이렇습니다.

가변 폭 인코딩인 UTF-8는 바이트의 개수가 곧 문자의 개수와 일치하지는 않는데요.
하지만 UTF-8로 인코딩된 바이트 시퀀스를 읽어서 문자의 개수를 세는 `strlen()`과 같은 함수를 만들어볼 수 있을 것입니다.
앞서 구현했던 디코딩 함수는 바이트 시퀀스로부터 코드 포인트를 리턴하는데요.
코드 포인트 대신에 그 개수를 리턴할 수 있을 것입니다.
(연습문제로 직접 해보세요.)

사실 UTF-8는 [URL 인코딩][wiki-url-enc]과도 관련이 있는데요.
URL에 '가'와 같은 문자를 사용하고 싶다면 그 UTF-8 인코딩 결과인 `EA B0 80`을 이용해 `%EA%B0%80`과 같은 식으로 변환합니다.

한편 문자 데이터를 정렬할 때는 문자의 순서를 생각해야 하는데요. (특히 데이터베이스에서요.)
코드 포인트가 반드시 문자의 순서라고 말할 수는 없습니다.
(문자 '0'이 문자 'A'보다 먼저일 필요가 있을까요?)
따라서 순서를 별도로 정의할 필요가 있는데요.
이를 콜레이션<AltLang>collation</AltLang>이라고 부릅니다.

(불행히도) 유니코드 상에서 어떤 문자는 나타내는 방법이 반드시 하나만 있는 것은 아닌데요.
예를 들어 악센트가 있는 A 문자 '&#192;' (`U+00C0`)를 봅시다.
이 문자는 두 문자를 조합해서 만들 수 있습니다.
문자 'A' (`U+0041`)에 악센트 조합 문자인 '◌̀' (`U+0300`)를 더해서 '&#65;&#768;'로요. (방금 실제로 두 문자를 조합했습니다. 똑같이 보이시나요?)

정규화<AltLang>normalization</AltLang>는 이런 문제를 위해 문자를 하나로 통일시키는 작업을 일컫습니다.
대표적으로 예를 들면 자바스크립트의 스트링 메소드인 `normalize()`가 그 정규화를 맡고 있습니다.

UTF-8을 비롯해 UTF-16과 UTF-32 방식도 있는데요.
기회가 되면 다음 글에서 다뤄보도록 하겠습니다.

[wiki-url-enc]: https://en.wikipedia.org/wiki/URL_encoding
